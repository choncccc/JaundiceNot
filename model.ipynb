{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL \n",
    "* Install muna ako NVIDIA CUDA tas cuDNN para di sumabog PC \n",
    "* Isang pack ng sweet&spicy pancit canton pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Images\n",
    "def load_images_and_labels(folder, label):\n",
    "    images = []\n",
    "    for file in os.listdir(folder):\n",
    "        image_path = os.path.join(folder, file)\n",
    "        image = tf.keras.utils.img_to_array(tf.keras.utils.load_img(image_path))\n",
    "        images.append(image.flatten())\n",
    "    labels = [label] * len(images)\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaundiced images shape: (399, 120000)\n",
      "Jaundiced features shape: (399, 24)\n",
      "Healthy images shape: (497, 120000)\n",
      "Healthy features shape: (497, 24)\n",
      "float64\n",
      "[[177. 114.  79. ... 158. 111.  85.]\n",
      " [235. 219. 203. ... 237. 227. 217.]\n",
      " [149. 100.  70. ... 109.  72.  45.]\n",
      " [ 69.  69.  71. ...   7.   1.   1.]\n",
      " [198. 144. 120. ... 209. 164. 135.]]\n"
     ]
    }
   ],
   "source": [
    "#converts array [R][G][B]\n",
    "def process_rgb_columns(features_df):\n",
    "    rgb_columns = ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5', 'Cluster 6']\n",
    "    processed_columns = []\n",
    "\n",
    "    for col in rgb_columns:\n",
    "        rgb_split = features_df[col].str.split(r'[-/]', expand=True)\n",
    "        rgb_split.columns = [f\"{col}_R\", f\"{col}_G\", f\"{col}_B\"]\n",
    "        \n",
    "        rgb_split = rgb_split.astype(int)\n",
    "        processed_columns.append(rgb_split)\n",
    "    processed_df = pd.concat(processed_columns, axis=1)\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "# Load images + labels\n",
    "train_jaundiced_images, train_jaundiced_labels = load_images_and_labels(\"./train/train J\", 1)\n",
    "train_healthy_images, train_healthy_labels = load_images_and_labels(\"./train/train N\", 0)\n",
    "\n",
    "\n",
    "train_jaundiced_features_df = pd.read_csv(\"./features/jaundiced_features.csv\")\n",
    "train_healthy_features_df = pd.read_csv(\"./features/healthy_features.csv\")\n",
    "\n",
    "train_jaundiced_features_df = train_jaundiced_features_df.drop(columns=['Image'])\n",
    "train_healthy_features_df = train_healthy_features_df.drop(columns=['Image'])\n",
    "\n",
    "train_jaundiced_processed_features = process_rgb_columns(train_jaundiced_features_df)\n",
    "train_healthy_processed_features = process_rgb_columns(train_healthy_features_df)\n",
    "\n",
    "train_jaundiced_features = pd.concat([train_jaundiced_features_df.reset_index(drop=True), train_jaundiced_processed_features], axis=1).values\n",
    "train_healthy_features = pd.concat([train_healthy_features_df.reset_index(drop=True), train_healthy_processed_features], axis=1).values\n",
    "\n",
    "\n",
    "train_jaundiced = np.hstack((train_jaundiced_images, train_jaundiced_features))\n",
    "train_healthy = np.hstack((train_healthy_images, train_healthy_features))\n",
    "\n",
    "X_train = np.vstack((train_jaundiced, train_healthy))\n",
    "y_train = np.hstack((train_jaundiced_labels, train_healthy_labels))\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=1)\n",
    "\n",
    "test_jaundiced_images, test_jaundiced_labels = load_images_and_labels(\"./test/test J\", 1)\n",
    "test_healthy_images, test_healthy_labels = load_images_and_labels(\"./test/test N\", 0)\n",
    "\n",
    "val_jaundiced_images, val_jaundiced_labels = load_images_and_labels(\"./validate/validate J\", 1)\n",
    "val_healthy_images, val_healthy_labels = load_images_and_labels(\"./validate/validate N\", 0)\n",
    "\n",
    "# Combine test and validation (no annotations)\n",
    "X_test = np.vstack((test_jaundiced_images, test_healthy_images))\n",
    "y_test = np.hstack((test_jaundiced_labels, test_healthy_labels))\n",
    "\n",
    "X_val = np.vstack((val_jaundiced_images, val_healthy_images))\n",
    "y_val = np.hstack((val_jaundiced_labels, val_healthy_labels))\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(X_train).apply(pd.to_numeric, errors='coerce').values\n",
    "X_test = pd.DataFrame(X_test).apply(pd.to_numeric, errors='coerce').values\n",
    "X_val = pd.DataFrame(X_val).apply(pd.to_numeric, errors='coerce').values\n",
    "\n",
    "X_train = X_train[:, :120000] #need to trim kasi may excess na 24 features diko alam kung saan galing\n",
    "\n",
    "#prent\n",
    "print(\"Jaundiced images shape:\", train_jaundiced_images.shape)\n",
    "print(\"Jaundiced features shape:\", train_jaundiced_features.shape)\n",
    "print(\"Healthy images shape:\", train_healthy_images.shape)\n",
    "print(\"Healthy features shape:\", train_healthy_features.shape)\n",
    "print(X_train.dtype)\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (896, 120000)\n",
      "X_test shape: (38, 120000)\n",
      "X_val shape: (18, 120000)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"X_val shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Fit \n",
    "X_test = scaler.transform(X_test)       # Transform test data (images only with no annotations)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenji\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_dim=X_train.shape[1]),  # Combined images + features\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary (J or N || 0 or 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 373ms/step - accuracy: 0.7890 - loss: 2.8349 - precision_3: 0.7697 - val_accuracy: 0.9444 - val_loss: 0.5027 - val_precision_3: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 364ms/step - accuracy: 0.7724 - loss: 2.7843 - precision_3: 0.7348 - val_accuracy: 0.8889 - val_loss: 0.5188 - val_precision_3: 0.8750 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 375ms/step - accuracy: 0.7934 - loss: 2.1161 - precision_3: 0.7417 - val_accuracy: 0.8333 - val_loss: 0.6777 - val_precision_3: 0.8571 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 362ms/step - accuracy: 0.8191 - loss: 2.1640 - precision_3: 0.7867 - val_accuracy: 0.8889 - val_loss: 0.8099 - val_precision_3: 0.8750 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 358ms/step - accuracy: 0.8243 - loss: 1.8206 - precision_3: 0.7872 - val_accuracy: 0.8889 - val_loss: 0.6620 - val_precision_3: 0.8750 - learning_rate: 1.0000e-05\n",
      "Epoch 6/30\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 355ms/step - accuracy: 0.8178 - loss: 1.7778 - precision_3: 0.7940 - val_accuracy: 0.8889 - val_loss: 0.6463 - val_precision_3: 0.8750 - learning_rate: 1.0000e-05\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8147 - loss: 2.7003 - precision_3: 0.8791\n",
      "Test Loss: 2.7746, Test Accuracy: 0.8158, Test Precision: 0.8571, Stopped at Epoch: 5, 125.7899\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', Precision()])\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True) # loss or accuracy??\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_val, y_val), callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "# Evaluate \n",
    "loss, accuracy, precision = model.evaluate(X_test, y_test)\n",
    "stopped = early_stopping.stopped_epoch\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}, Test Precision: {precision:.4f}, Stopped at Epoch: {stopped}, {elapsed_time:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected. Ensure proper installation of TensorFlow, CUDA, and cuDNN.\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs available:\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected. Ensure proper installation of TensorFlow, CUDA, and cuDNN.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
